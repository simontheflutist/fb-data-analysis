{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MESSAGES_JSON_PATH = 'data/message_1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_messages(high_unicode=False):\n",
    "    \"\"\"\n",
    "    Return an iterator with sender names and message content.\n",
    "    \"\"\"\n",
    "    with open(MESSAGES_JSON_PATH, 'r') as json_file:\n",
    "        for message in json.loads(\n",
    "                json_file\n",
    "                .read())['messages']:\n",
    "            if message['type'] == 'Generic':\n",
    "                try:\n",
    "                    sender_name = message['sender_name']\n",
    "                    content = message['content']                   \n",
    "                except KeyError:\n",
    "                    continue\n",
    "                # clobber the emojis; they're hard to work with\n",
    "                try:\n",
    "                    if high_unicode:\n",
    "                        content = [c for c in content\n",
    "                                   if ord(c) > 126]\n",
    "                    else:\n",
    "                        content = content.encode('ascii',\n",
    "                                                 errors='ignore')\n",
    "                        content = str(content,\n",
    "                                      encoding='ascii')\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if content:\n",
    "                    yield sender_name, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_messages():\n",
    "    yield from (msg for _, msg in load_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_by_sender():\n",
    "    corpus_by_sender = defaultdict(list)\n",
    "    for sender, msg in load_messages():\n",
    "        corpus_by_sender[sender].append(msg)\n",
    "    return corpus_by_sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags():\n",
    "    \"\"\"Returns a defaultdict tally of all substrings of the\n",
    "        form`@spam`.\"\"\"\n",
    "    tag_matcher = re.compile('\\@[A-Za-z]+')\n",
    "    tags = defaultdict(int)\n",
    "    for msg in get_all_messages():\n",
    "        for tag in tag_matcher.findall(msg):\n",
    "            tags[tag] += 1\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_dict(d):\n",
    "    return sorted(((v, k) \n",
    "                   for (k, v)\n",
    "                   in d.items()),\n",
    "                  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_frequencies():\n",
    "    \"\"\"Return a dict()\n",
    "       with log frequencies of all word lemmas.\"\"\"\n",
    "    # tally\n",
    "    word_matcher = re.compile(\"[A-Za-z']+\")\n",
    "    tally = defaultdict(int)\n",
    "    total = 0\n",
    "    for msg in get_all_messages():\n",
    "        for match in word_matcher.findall(msg):\n",
    "            total += 1\n",
    "            lemma = match.lower()\n",
    "            tally[lemma] += 1\n",
    "    return {lemma: count / total\n",
    "            for lemma, count\n",
    "            in tally.items()\n",
    "            if count > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_English_frequencies():\n",
    "    kv_line_matcher = re.compile(r\"^\\s*(\\w+),(\\w+)$\",\n",
    "                                 re.MULTILINE)\n",
    "    tally = dict()\n",
    "    total = 0\n",
    "    with open('data/english_counts.csv', 'r') as f:\n",
    "        csv_text = f.read()\n",
    "        kv_line_matches \\\n",
    "            = kv_line_matcher.finditer(csv_text)\n",
    "        for match in kv_line_matches:\n",
    "            lemma = match[1].lower()\n",
    "            count = float(match[2])\n",
    "            total += count\n",
    "            tally[lemma] = count\n",
    "    return {lemma: count / total\n",
    "            for lemma, count\n",
    "            in tally.items()\n",
    "            if count > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea was to compare either KL divergence or cross-entropy\n",
    "# summand to standard English to see which words we use\n",
    "# differently from the general English corpus;\n",
    "# overall I think it more reflects the differences in chat\n",
    "# English from regular English than anything about \n",
    "# this particular chat\n",
    "def get_KL_divergences():\n",
    "    english_freqs = get_English_frequencies()\n",
    "    KL_divs = dict()\n",
    "    for lemma, lemma_freq \\\n",
    "            in get_lemma_frequencies().items():\n",
    "        try:\n",
    "            normal_freq = english_freqs[lemma]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        KL_divs[lemma] = (lemma_freq \n",
    "                          * (np.log(lemma_freq)\n",
    "                             - np.log(normal_freq)))\n",
    "    return KL_divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_by_sender(high_unicode=False):\n",
    "    corpus_by_sender = defaultdict(list)\n",
    "    for sender, msg in load_messages(high_unicode):\n",
    "        corpus_by_sender[sender].append(msg)\n",
    "    return corpus_by_sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_characters(high_unicode=False):\n",
    "    return {name: sum(map(len, messages))\n",
    "            for name, messages\n",
    "            in get_corpus_by_sender(high_unicode).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
